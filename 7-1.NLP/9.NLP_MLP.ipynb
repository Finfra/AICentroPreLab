{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from konlpy.tag import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All string : 나는 너를 사랑한다. 나는 나를 사랑한다. 나는 나를 증오한다. 나는 너를 증오한다. 너는 너를 증오한다.\n",
      "All tockents: ['나', '너', '사랑', '나', '나', '사랑', '나', '나', '증오', '나', '너', '증오', '너', '너', '증오']\n",
      "Dictionary :  {'나': 0, '너': 1, '사랑': 2, '증오': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>나는 너를 사랑한다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나는 나를 사랑한다.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>나는 나를 증오한다.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>나는 너를 증오한다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>너는 너를 증오한다.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       corpus  label  0  1  2  3\n",
       "0   0  나는 너를 사랑한다.      1  1  1  1  0\n",
       "1   1  나는 나를 사랑한다.      0  2  0  1  0\n",
       "2   2  나는 나를 증오한다.      0  2  0  0  1\n",
       "3   3  나는 너를 증오한다.      1  1  1  0  1\n",
       "4   4  너는 너를 증오한다.      1  0  2  0  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus= [\n",
    "    \"나는 너를 사랑한다.\",\n",
    "    \"나는 나를 사랑한다.\",\n",
    "    \"나는 나를 증오한다.\",\n",
    "    \"나는 너를 증오한다.\",\n",
    "    \"너는 너를 증오한다.\"\n",
    "    ]\n",
    "df=pd.DataFrame({\"id\": range(len(corpus)),\"corpus\": corpus,\"label\":[1,0,0,1,1]\n",
    "})                 \n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "s=\" \".join(df.corpus)\n",
    "\n",
    "print('All string :' ,s)\n",
    "\n",
    "# posToUse=[\"NNP\",\"NNG\",\"MAG\",\"NP\",\"VV\",\"VV+EF\",\"IC\"]\n",
    "posToUse=[\"NNP\",\"NNG\",\"NP\",\"NNB\"]\n",
    "\n",
    "def getTokens(s):\n",
    "    global posToUse\n",
    "    return [ i[0] for i in  Mecab().pos(s) if i[1] in posToUse ] \n",
    "    \n",
    "print('All tockents:', getTokens(s))\n",
    "\n",
    "posToUse=[\"NNP\",\"NNG\",\"NP\",\"NNB\"]\n",
    "stopwords=[\"놈\",\"18\"]\n",
    "def getToken(s,pos=posToUse,stopword=stopwords):\n",
    "    return [ w for w,t in  Mecab().pos(s) if t in pos and w not in stopword ] \n",
    "vect = CountVectorizer(tokenizer=getToken)\n",
    "vect.fit(getTokens(s))\n",
    "print('Dictionary : ',vect.vocabulary_)\n",
    "cd=pd.DataFrame(vect.transform([\" \".join(getTokens(i)) for i in df.corpus]).toarray())\n",
    "data=pd.concat([df,cd], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "x_train=data.iloc[:,3:11].to_numpy()\n",
    "#y_train=data.label.to_numpy().reshape([-1,1])\n",
    "y_train=keras.utils.to_categorical(data.label,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 14)                70        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 60        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 350\n",
      "Trainable params: 350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(14, activation='tanh', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(14, activation='tanh'))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.7742 - accuracy: 0.4000 - val_loss: 0.7292 - val_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7296 - accuracy: 0.4000 - val_loss: 0.7066 - val_accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6990 - accuracy: 0.6000 - val_loss: 0.6835 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6776 - accuracy: 0.6000 - val_loss: 0.6612 - val_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6601 - accuracy: 0.6000 - val_loss: 0.6406 - val_accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6344 - accuracy: 0.6000 - val_loss: 0.6231 - val_accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6196 - accuracy: 0.6000 - val_loss: 0.6053 - val_accuracy: 0.6000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6016 - accuracy: 0.6000 - val_loss: 0.5889 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5847 - accuracy: 0.6000 - val_loss: 0.5734 - val_accuracy: 0.6000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5688 - accuracy: 0.6000 - val_loss: 0.5591 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5591359734535217\n",
      "Train accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49857116, 0.5014288 ],\n",
       "       [0.6118893 , 0.3881107 ],\n",
       "       [0.45008156, 0.5499185 ],\n",
       "       [0.35390595, 0.646094  ],\n",
       "       [0.31548625, 0.6845137 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-22ac85ef19b4>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
