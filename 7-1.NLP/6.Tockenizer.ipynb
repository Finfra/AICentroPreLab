{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "posToUse=[\"NNP\",\"NNG\",\"MAG\",\"NP\",\"VV\",\"VV+EF\",'XSV+EC']\n",
    "\n",
    "def getTokens(s):\n",
    "    global posToUse\n",
    "    return \" \".joi\n",
    "\n",
    "n([ i[0] for i in  Mecab().pos(s) if i[1] in posToUse ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Version by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '나를', '사랑한다']\n",
      "['나', '너', '사랑', '한다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "s=\"나는 너를 사랑한다\"\n",
    "\n",
    "print(word_tokenize(\"나는 나를 사랑한다\"))  \n",
    "print(word_tokenize(getTokens(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All string : 고인물은 게임을 즐긴다. 남중호는 게임을 잘한다. 남중구는 게임을 못한다. 나는 나란 놈 사랑한다.18 너의 게임 나의 게임 사랑하라\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"고인물은 게임을 즐긴다.\",\n",
    "    \"남중호는 게임을 잘한다.\",\n",
    "    \"남중구는 게임을 못한다.\",\n",
    "    \"나는 나란 놈 사랑한다.18\",\n",
    "    \"너의 게임 나의 게임 사랑하라\"\n",
    "]\n",
    "s=\" \".join(corpus)\n",
    "print('All string :' ,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 6], [7, 8, 1, 9, 2], [10, 1, 11, 2], [3, 3, 4, 2], [12, 1, 3, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "sentences=[getTokens(r) for r in corpus]\n",
    "tokenizer.fit_on_texts (sentences)\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'게임': 1,\n",
       " '한다': 2,\n",
       " '나': 3,\n",
       " '사랑': 4,\n",
       " '인물': 5,\n",
       " '즐긴다': 6,\n",
       " '남': 7,\n",
       " '중호': 8,\n",
       " '잘': 9,\n",
       " '남중구': 10,\n",
       " '못': 11,\n",
       " '너': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  5,  1,  6],\n",
       "       [ 7,  8,  1,  9,  2],\n",
       "       [ 0, 10,  1, 11,  2],\n",
       "       [ 0,  3,  3,  4,  2],\n",
       "       [12,  1,  3,  1,  4]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sentences=[getTokens(r) for r in corpus]\n",
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "pad_sequences(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  1,  6,  0,  0],\n",
       "       [ 7,  8,  1,  9,  2],\n",
       "       [10,  1, 11,  2,  0],\n",
       "       [ 3,  3,  4,  2,  0],\n",
       "       [12,  1,  3,  1,  4]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(encoded,padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cf) Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. Nam은 게임을 즐긴다.', '남중호는 게임을 잘한다...그런가 보다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Mr. Nam은 게임을 즐긴다. 남중호는 게임을 잘한다...그런가 보다.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Transforms each text in texts to a sequence of integers.\n",
       "\n",
       "Only top `num_words-1` most frequent words will be taken into account.\n",
       "Only words known by the tokenizer will be taken into account.\n",
       "\n",
       "# Arguments\n",
       "    texts: A list of texts (strings).\n",
       "\n",
       "# Returns\n",
       "    A list of sequences.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tokenizer.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
