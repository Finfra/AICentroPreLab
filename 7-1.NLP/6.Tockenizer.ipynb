{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '너', '사랑', '한다']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "posToUse=[\"NNP\",\"NNG\",\"MAG\",\"NP\",\"VV\",\"VV+EF\",'XSV+EC']\n",
    "\n",
    "def getTokens(s):\n",
    "    global posToUse\n",
    "    return \" \".join(s)\n",
    "\n",
    "s=\"나는 너를 사랑한다\"\n",
    "print([ i[0] for i in  Mecab().pos(s) if i[1] in posToUse ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Version by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '나를', '사랑한다']\n",
      "['나', '는', '너', '를', '사', '랑', '한', '다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "s=\"나는 너를 사랑한다\"\n",
    "\n",
    "print(word_tokenize(\"나는 나를 사랑한다\"))  \n",
    "print(word_tokenize(getTokens(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All string : 고인물은 게임을 즐긴다. 남중호는 게임을 잘한다. 남중구는 게임을 못한다. 나는 나란 놈 사랑한다.18 너의 게임 나의 게임 사랑하라\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"고인물은 게임을 즐긴다.\",\n",
    "    \"남중호는 게임을 잘한다.\",\n",
    "    \"남중구는 게임을 못한다.\",\n",
    "    \"나는 나란 놈 사랑한다.18\",\n",
    "    \"너의 게임 나의 게임 사랑하라\"\n",
    "]\n",
    "s=\" \".join(corpus)\n",
    "print('All string :' ,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 14, 15, 16, 1, 2, 4, 17, 18, 3], [8, 9, 19, 5, 1, 2, 4, 20, 6, 3], [8, 9, 21, 5, 1, 2, 4, 22, 6, 3], [7, 5, 7, 23, 24, 10, 11, 6, 3, 25, 26], [27, 12, 1, 2, 7, 12, 1, 2, 10, 11, 28, 29]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "sentences=[getTokens(r) for r in corpus]\n",
    "tokenizer.fit_on_texts (sentences)\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'게': 1,\n",
       " '임': 2,\n",
       " '다': 3,\n",
       " '을': 4,\n",
       " '는': 5,\n",
       " '한': 6,\n",
       " '나': 7,\n",
       " '남': 8,\n",
       " '중': 9,\n",
       " '사': 10,\n",
       " '랑': 11,\n",
       " '의': 12,\n",
       " '고': 13,\n",
       " '인': 14,\n",
       " '물': 15,\n",
       " '은': 16,\n",
       " '즐': 17,\n",
       " '긴': 18,\n",
       " '호': 19,\n",
       " '잘': 20,\n",
       " '구': 21,\n",
       " '못': 22,\n",
       " '란': 23,\n",
       " '놈': 24,\n",
       " '1': 25,\n",
       " '8': 26,\n",
       " '너': 27,\n",
       " '하': 28,\n",
       " '라': 29}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 13, 14, 15, 16,  1,  2,  4, 17, 18,  3],\n",
       "       [ 0,  0,  8,  9, 19,  5,  1,  2,  4, 20,  6,  3],\n",
       "       [ 0,  0,  8,  9, 21,  5,  1,  2,  4, 22,  6,  3],\n",
       "       [ 0,  7,  5,  7, 23, 24, 10, 11,  6,  3, 25, 26],\n",
       "       [27, 12,  1,  2,  7, 12,  1,  2, 10, 11, 28, 29]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sentences=[getTokens(r) for r in corpus]\n",
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "pad_sequences(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 14, 15, 16,  1,  2,  4, 17, 18,  3,  0,  0],\n",
       "       [ 8,  9, 19,  5,  1,  2,  4, 20,  6,  3,  0,  0],\n",
       "       [ 8,  9, 21,  5,  1,  2,  4, 22,  6,  3,  0,  0],\n",
       "       [ 7,  5,  7, 23, 24, 10, 11,  6,  3, 25, 26,  0],\n",
       "       [27, 12,  1,  2,  7, 12,  1,  2, 10, 11, 28, 29]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(encoded,padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cf) Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. Nam은 게임을 즐긴다.', '남중호는 게임을 잘한다...그런가 보다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Mr. Nam은 게임을 즐긴다. 남중호는 게임을 잘한다...그런가 보다.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Transforms each text in texts to a sequence of integers.\n",
       "\n",
       "Only top `num_words-1` most frequent words will be taken into account.\n",
       "Only words known by the tokenizer will be taken into account.\n",
       "\n",
       "# Arguments\n",
       "    texts: A list of texts (strings).\n",
       "\n",
       "# Returns\n",
       "    A list of sequences.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tokenizer.texts_to_sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
